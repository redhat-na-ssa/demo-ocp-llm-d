apiVersion: v1
kind: Pod
metadata:
  # generateName: guidellm-
  name: guidellm
spec:
  containers:
  - name: extended-resource-demo
    image: ghcr.io/vllm-project/guidellm:v0.3.0
    # image: registry.redhat.io/rhel8/python-311
    command:
      - /bin/bash
      - -c
      - |
        #!/bin/sh

        debug(){
          echo "To infinity and beyond..."
          sleep infinity
        }

        cat <<EOF > /tmp/functions
        run_guidellm(){

          # GUIDELLM_TARGET=http://llm-service:8000
          # GUIDELLM_RATE_TYPE=sweep
          # GUIDELLM_MAX_SECONDS=30
          # GUIDELLM_DATA="prompt_tokens=256,output_tokens=128"

          guidellm benchmark run \
            --target http://gpt-oss-20b-predictor \
            --model gpt-oss-20b \
            --data "prompt_tokens=512, output_tokens=256" \
            --rate-type sweep \
            --max-seconds 30 \
            --processor /config \
            --output-path /results
        }
        EOF

        . /tmp/functions

        # wait for files to be ready
        until [ -e /config/tokenizer.json ]; do : ; done

        # run_guidellm
        debug

    volumeMounts:
      - mountPath: /data
        name: data
      - mountPath: /config
        # name: data
        name: empty
        subPath: config
      - mountPath: /results
        name: empty
  volumes:
    - name: empty
      emptyDir: {}
    - name: data
      persistentVolumeClaim:
        claimName: guidellm-data
        # defaultMode: 775
